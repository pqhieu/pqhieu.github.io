<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pham Quang Hieu</title>
    <link>https://pqhieu.com/</link>
    <description>Recent content on Pham Quang Hieu</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Jun 2020 19:23:54 +0800</lastBuildDate>
    <atom:link href="https://pqhieu.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields</title>
      <link>https://pqhieu.com/research/cvpr19/</link>
      <pubDate>Mon, 29 Jun 2020 19:23:54 +0800</pubDate>
      <guid>https://pqhieu.com/research/cvpr19/</guid>
      <description>&lt;h1 id=&#34;jsis3d-joint-semantic-instance-segmentation-of-3d-point-clouds-with-multi-task-pointwise-networks-and-multi-value-conditional-random-fields&#34;&gt;JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields&lt;/h1&gt;&#xA;&lt;p&gt;&#xA;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;&#xA;&#xA;&lt;em&gt;Quang-Hieu Pham&lt;/em&gt;&lt;sup&gt;1&lt;/sup&gt;  &#xA;&lt;em&gt;Binh-Son Hua&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;  &#xA;&lt;em&gt;Duc Thanh Nguyen&lt;/em&gt;&lt;sup&gt;3&lt;/sup&gt;  &#xA;&lt;em&gt;Gemma Roig&lt;/em&gt;&lt;sup&gt;1&lt;/sup&gt;  &#xA;&lt;em&gt;Sai-Kit Yeung&lt;/em&gt;&lt;sup&gt;4&lt;/sup&gt;  &#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;&#xA;&#xA;&lt;sup&gt;1&lt;/sup&gt;Singapore University of Technology and Design  &#xA;&lt;sup&gt;2&lt;/sup&gt;The University of Tokyo  &lt;br&gt;&#xA;&lt;sup&gt;3&lt;/sup&gt;Deadkin University  &#xA;&lt;sup&gt;4&lt;/sup&gt;Hong Kong University of Science and Technology&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;&#xA;&#xA;IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.&#xA;&lt;span class=&#34;smcp&#34;&gt;Oral&lt;/span&gt;.&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;&#xA;&#xA;&lt;a class=&#34;button&#34; href=&#34;https://arxiv.org/pdf/1904.00699.pdf&#34;&gt;&#xA;&lt;span class=&#34;icon&#34;&gt;&#xA;&lt;i class=&#34;fas fa-file-pdf&#34;&gt;&lt;/i&gt;&#xA;&lt;/span&gt;&#xA;&lt;/a&gt;&#xA;&lt;a class=&#34;button&#34; href=&#34;https://github.com/pqhieu/jsis3d&#34;&gt;&#xA;&lt;span class=&#34;icon&#34;&gt;&#xA;&lt;i class=&#34;fab fa-github&#34;&gt;&lt;/i&gt;&#xA;&lt;/span&gt;&#xA;&lt;/a&gt;&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;&#xA;&lt;p&gt;&lt;span class=&#34;smcp&#34;&gt;Deep learning techniques&lt;/span&gt; have become the to-go models for most vision-related tasks on 2D images.&#xA;However, their power has not been fully realised on several tasks in 3D space, e.g., 3D scene understanding.&#xA;In this work, we jointly address the problems of semantic and instance segmentation of 3D point clouds.&#xA;Specifically, we develop a multi-task pointwise network that simultaneously performs two tasks: predicting the semantic classes of 3D points and embedding the points into high-dimensional vectors so that points of the same object instance are represented by similar embeddings.&#xA;We then propose a multi-value conditional random field model to incorporate the semantic and instance labels and formulate the problem of semantic and instance segmentation as jointly optimising labels in the field model.&#xA;The proposed method is thoroughly evaluated and compared with existing methods on different indoor scene datasets including S3DIS and SceneNN.&#xA;Experimental results showed the robustness of the proposed joint semantic-instance segmentation scheme over its single components.&#xA;Our method also achieved state-of-the-art performance on semantic segmentation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LCD: Learned Cross-domain Descriptors for 2D-3D Matching</title>
      <link>https://pqhieu.com/research/aaai20/</link>
      <pubDate>Mon, 29 Jun 2020 19:23:54 +0800</pubDate>
      <guid>https://pqhieu.com/research/aaai20/</guid>
      <description>&lt;h1 id=&#34;lcd-learned-cross-domain-descriptors-for-2d-3d-matching&#34;&gt;LCD: Learned Cross-domain Descriptors for 2D-3D Matching&lt;/h1&gt;&#xA;&lt;p&gt;&#xA;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;&#xA;&#xA;&lt;em&gt;Quang-Hieu Pham&lt;/em&gt;&lt;sup&gt;1&lt;/sup&gt;  &#xA;&lt;em&gt;Mikaela Angelina Uy&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;  &#xA;&lt;em&gt;Binh-Son Hua&lt;/em&gt;&lt;sup&gt;3&lt;/sup&gt;  &#xA;&lt;em&gt;Duc Thanh Nguyen&lt;/em&gt;&lt;sup&gt;4&lt;/sup&gt;  &lt;br&gt;&#xA;&lt;em&gt;Gemma Roig&lt;/em&gt;&lt;sup&gt;5&lt;/sup&gt;  &#xA;&lt;em&gt;Sai-Kit Yeung&lt;/em&gt;&lt;sup&gt;6&lt;/sup&gt;  &#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;&#xA;&#xA;&lt;sup&gt;1&lt;/sup&gt;Singapore University of Technology and Design  &#xA;&lt;sup&gt;2&lt;/sup&gt;Stanford University  &#xA;&lt;sup&gt;3&lt;/sup&gt;The University of Tokyo  &lt;br&gt;&#xA;&lt;sup&gt;4&lt;/sup&gt;Deadkin University   &#xA;&lt;sup&gt;5&lt;/sup&gt;Geothe University of Frankfrut  &#xA;&lt;sup&gt;6&lt;/sup&gt;Hong Kong University of Science and Technology&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;&#xA;&#xA;AAAI Conference on Artificial Intelligence (AAAI), 2020. &lt;span class=&#34;smcp&#34;&gt;Oral&lt;/span&gt;.&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;&#xA;&#xA;&lt;a class=&#34;button&#34; href=&#34;https://arxiv.org/pdf/1911.09326.pdf&#34;&gt;&#xA;&lt;span class=&#34;icon&#34;&gt;&#xA;&lt;i class=&#34;fas fa-file-pdf&#34;&gt;&lt;/i&gt;&#xA;&lt;/span&gt;&#xA;&lt;/a&gt;&#xA;&lt;a class=&#34;button&#34; href=&#34;https://github.com/hkust-vgd/lcd&#34;&gt;&#xA;&lt;span class=&#34;icon&#34;&gt;&#xA;&lt;i class=&#34;fab fa-github&#34;&gt;&lt;/i&gt;&#xA;&lt;/span&gt;&#xA;&lt;/a&gt;&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;&#xA;&lt;div class=&#34;video&#34;&gt;&#xA;      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/sMEmqfjLDZw?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;&#xA;    &lt;/div&gt;&#xA;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;&#xA;&lt;p&gt;&lt;span class=&#34;smcp&#34;&gt;In this work&lt;/span&gt;, we present a novel method to learn a local cross-domain descriptor for 2D image and 3D point cloud matching.&#xA;Our proposed method is a dual auto-encoder neural network that maps 2D and 3D input into a shared latent space representation.&#xA;We show that such local cross-domain descriptors in the shared embedding are more discriminative than those obtained from individual training in 2D and 3D domains.&#xA;To facilitate the training process, we built a new dataset by collecting ≈1.4 millions of 2D-3D correspondences with various lighting conditions and settings from publicly available RGB-D scenes.&#xA;Our descriptor is evaluated in three main experiments: 2D-3D matching, cross-domain retrieval, and sparse-to-dense depth estimation.&#xA;Experimental results confirm the robustness of our approach as well as its competitive performance not only in solving cross-domain tasks but also in being able to generalize to solve sole 2D and 3D tasks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Real-time Progressive 3D Semantic Segmentation for Indoor Scenes</title>
      <link>https://pqhieu.com/research/wacv19/</link>
      <pubDate>Mon, 29 Jun 2020 19:23:54 +0800</pubDate>
      <guid>https://pqhieu.com/research/wacv19/</guid>
      <description>&lt;h1 id=&#34;real-time-progressive-3d-semantic-segmentation-for-indoor-scenes&#34;&gt;Real-time Progressive 3D Semantic Segmentation for Indoor Scenes&lt;/h1&gt;&#xA;&lt;p&gt;&#xA;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;&#xA;&#xA;&lt;em&gt;Quang-Hieu Pham&lt;/em&gt;&lt;sup&gt;1&lt;/sup&gt;  &#xA;&lt;em&gt;Binh-Son Hua&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;  &#xA;&lt;em&gt;Duc Thanh Nguyen&lt;/em&gt;&lt;sup&gt;3&lt;/sup&gt;  &#xA;&lt;em&gt;Sai-Kit Yeung&lt;/em&gt;&lt;sup&gt;4&lt;/sup&gt;  &#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;&#xA;&#xA;&lt;sup&gt;1&lt;/sup&gt;Singapore University of Technology and Design  &#xA;&lt;sup&gt;2&lt;/sup&gt;The University of Tokyo  &#xA;&lt;sup&gt;3&lt;/sup&gt;Deadkin University  &lt;br&gt;&#xA;&lt;sup&gt;4&lt;/sup&gt;Hong Kong University of Science and Technology&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;&#xA;&#xA;IEEE Winter Conference on Applications of Computer Vision (WACV), 2019.&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;&#xA;&#xA;&lt;a class=&#34;button&#34; href=&#34;https://arxiv.org/pdf/1804.00257.pdf&#34;&gt;&#xA;&lt;span class=&#34;icon&#34;&gt;&#xA;&lt;i class=&#34;fas fa-file-pdf&#34;&gt;&lt;/i&gt;&#xA;&lt;/span&gt;&#xA;&lt;/a&gt;&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;/p&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;&#xA;&lt;p&gt;&lt;span class=&#34;smcp&#34;&gt;The widespread adoption of autonomous systems&lt;/span&gt; such as drones and assistant robots has created a need for real-time high-quality semantic scene segmentation.&#xA;In this paper, we propose an efficient yet robust technique for on-the-fly dense reconstruction and semantic segmentation of 3D indoor scenes.&#xA;To guarantee (near) real-time performance, our method is built atop an efficient super-voxel clustering method and a conditional random field with higher-order constraints from structural and object cues, enabling progressive dense semantic segmentation without any precomputation.&#xA;We extensively evaluate our method on different indoor scenes including kitchens, offices, and bedrooms in the SceneNN and ScanNet datasets and show that our technique consistently produces state-of-the-art segmentation results in both qualitative and quantitative experiments.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
